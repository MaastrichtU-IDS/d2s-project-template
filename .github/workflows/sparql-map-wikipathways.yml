name: Convert WikiPathways to BioLink RDF
# TODO: Add step to manage versioning (delete previous graph, load new graph, generate metadata)
on:
  workflow_dispatch:
    inputs:
      endpoint:
        description: 'Upload to SPARQL endpoint'
        required: true
        default: 'https://graphdb.dumontierlab.com/repositories/ncats-red-kg/statements'
      graph:
        description: 'In the Graph'
        required: true
        default: 'https://w3id.org/d2s/graph/wikipathways'

jobs:
  run-sparql:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2

    - name: Run SPARQL queries to convert Wikipathways
      uses: vemonet/sparql-operations-action@v1
      with:
        file: datasets/wikipathways/mapping/*.rq
        endpoint: ${{ github.event.inputs.endpoint }}
        user: ${{ secrets.GRAPHDB_USER }}
        password: ${{ secrets.GRAPHDB_PASSWORD }}
        inputvar: http://rdf.wikipathways.org/
        outputvar: ${{ github.event.inputs.graph }}
        servicevar: http://sparql.wikipathways.org/sparql

    - name: Compute and insert HCLS descriptive metadata
      uses: vemonet/sparql-operations-action@v1
      with:
        file: https://github.com/MaastrichtU-IDS/d2s-scripts-repository/tree/master/sparql/compute-hcls-stats
        endpoint: ${{ github.event.inputs.endpoint }}
        user: ${{ secrets.GRAPHDB_USER }}
        password: ${{ secrets.GRAPHDB_PASSWORD }}
        inputvar: ${{ github.event.inputs.graph }}
        outputvar: https://w3id.org/d2s/metadata
        servicevar: ${{ github.event.inputs.endpoint }}
        # servicevar: http://localhost:7200/repositories/ncats-red-kg
